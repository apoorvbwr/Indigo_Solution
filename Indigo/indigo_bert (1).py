# -*- coding: utf-8 -*-
"""Indigo Bert.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nxpNZgRuY9fY9FmCfqnatRc4L3AM2Vnw
"""

pip install datasets

import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import spacy
from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer
from datasets import load_dataset

dataset = load_dataset("toughdata/quora-question-answer-dataset")

split_dataset = dataset["train"].train_test_split(test_size=0.2)
train_dataset = split_dataset["train"]
test_dataset = split_dataset["test"]
print(f"Number of samples in training set: {len(train_dataset)}")
print(f"Number of samples in test set: {len(test_dataset)}")

import pandas as pd

df = pd.read_json("hf://datasets/toughdata/quora-question-answer-dataset/Quora-QuAD.jsonl", lines=True)

import pandas as pd
from sklearn.model_selection import train_test_split

# Load the dataset
df = pd.read_json("hf://datasets/toughdata/quora-question-answer-dataset/Quora-QuAD.jsonl", lines=True)

# Convert the DataFrame to a CSV file
df.to_csv("Quora-QuAD.csv", index=False)

df = pd.read_csv("/content/Quora-QuAD.csv")
df.head()

# Remove duplicates
df = df.drop_duplicates(subset=["question", "answer"])# Check duplicate questions and remove them

print(f"Number of duplicate questions: {len(df) - df['question'].nunique()}")

df.drop_duplicates(subset=['question'],inplace=True)
print("\nNumber of records after removing duplicates: ",len(df))

print(df)

# Split the dataset into training and testing sets
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)

# Rename the training portion to dataset2
train_dataset = train_df

# If you want to also have the test portion
test_dataset = test_df

# Verify the split
print(f"Number of samples in training set: {len(train_dataset)}")
print(f"Number of samples in test set: {len(test_dataset)}")

print(df['question'][1])

print(df['answer'][1])

import re

def clean_text(text):
  text = text.lower()     # Lowercase
  text = re.sub(r'\[.*?\]', '',text)  # Remove square brackets
  text = re.sub(r'http\S+', '', text) # Remove URLs
  text = re.sub(r'<.*?>+', '', text)  # Remove HTML Tags
  text = re.sub(r'[^a-z\s]', '', text)  # Remove non-alphabetic characters
  text = re.sub(r'\s+', ' ', text).strip()  # Remove extra whitespaces
  return text

train_dataset['question'] = train_dataset['question'].apply(clean_text)
train_dataset['answer'] = train_dataset['answer'].apply(clean_text)

train_dataset

nltk.download('punkt')
train_dataset['question_tokens'] = train_dataset['question'].apply(word_tokenize)
train_dataset['answer_tokens'] = train_dataset['answer'].apply(word_tokenize)

nltk.download("stopwords")
stop_words = set(stopwords.words('english'))

def remove_stopwords(tokens):
  return [word for word in tokens if word not in stop_words]

train_dataset['question_tokens'] = train_dataset['question_tokens'].apply(remove_stopwords)
train_dataset['answer_tokens'] = train_dataset['answer_tokens'].apply(remove_stopwords)

nltk.download('wordnet')
lemmatizer = WordNetLemmatizer()

def lemmatize_tokens(tokens):
  return [lemmatizer.lemmatize(token) for token in tokens]

train_dataset['question_tokens'] = train_dataset['question_tokens'].apply(lemmatize_tokens)
train_dataset['answer_tokens'] = train_dataset['answer_tokens'].apply(lemmatize_tokens)

# Convert token lists back to strings for processing with spaCy
train_dataset['question_processed'] = train_dataset['question_tokens'].apply(lambda x: ' '.join(x))
train_dataset['answer_processed'] = train_dataset['answer_tokens'].apply(lambda x: ' '.join(x))

train_dataset

df2 = train_dataset[['question_processed','answer_processed']]
df2.head()

from transformers import BertTokenizer, BertForQuestionAnswering, TrainingArguments, Trainer

# Load the tokenizer and model
model_name = 'bert-base-uncased'  # Use a smaller model
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertForQuestionAnswering.from_pretrained(model_name)

# # Tokenize the dataset
# def preprocess_function(examples):
#     questions = examples['question_processed']
#     contexts = examples['answer_processed']  # Assuming you have a context column; otherwise, use the answer column directly
#     inputs = tokenizer(questions, contexts, truncation=True, padding='max_length', max_length=256, return_tensors='pt')
#     return inputs

# # Tokenize the dataset
def preprocess_function(examples):
    inputs = tokenizer(
        examples['question_processed'],
        examples['answer_processed'],  # Ensure 'context' column is used correctly
        truncation='longest_first',
        padding='max_length',
        max_length=256,
        return_tensors='pt'
    )
    inputs['labels'] = tokenizer(
        examples['answer'],
        truncation=True,
        padding='max_length',
        max_length=256
    )['input_ids']
    return inputs



from datasets import Dataset, load_metric
import numpy as np
import torch

dataset = Dataset.from_pandas(df2)

dataset

tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=['question_processed', 'answer_processed'])

# Load metrics
rouge = load_metric('rouge')
bleu = load_metric('bleu')

def compute_f1(preds, labels):
    preds = np.argmax(preds, axis=-1)
    true_positives = np.sum((preds == 1) & (labels == 1))
    false_positives = np.sum((preds == 1) & (labels == 0))
    false_negatives = np.sum((preds == 0) & (labels == 1))

    precision = true_positives / (true_positives + false_positives)
    recall = true_positives / (true_positives + false_negatives)
    f1 = 2 * (precision * recall) / (precision + recall)
    return f1

# Define the evaluation function
def compute_metrics(p):
    preds, labels = p
    preds = np.argmax(preds, axis=1)

    # Compute ROUGE
    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    rouge_result = rouge.compute(predictions=decoded_preds, references=decoded_labels)

    # Compute BLEU
    bleu_result = bleu.compute(predictions=[pred.split() for pred in decoded_preds], references=[[ref.split()] for ref in decoded_labels])

    # Compute F1-score
    f1_result = compute_f1(preds, labels)

    return {
        'rouge1': rouge_result['rouge1'].mid.fmeasure,
        'rouge2': rouge_result['rouge2'].mid.fmeasure,
        'rougeL': rouge_result['rougeL'].mid.fmeasure,
        'bleu': bleu_result['bleu'],
        'f1': f1_result
    }

# Fine-tune the model
training_args = TrainingArguments(
    output_dir='./results',
    evaluation_strategy="epoch",
    learning_rate=3e-5,  # You can also try slightly higher learning rates
    per_device_train_batch_size=16,  # Increase batch size
    per_device_eval_batch_size=16,
    num_train_epochs=1,  # Reduce number of epochs
    weight_decay=0.01,
    fp16=True,  # Enable mixed precision training
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    eval_dataset=tokenized_dataset,
    compute_metrics=compute_metrics,
)

trainer.train()

# Save the model
model.save_pretrained('./fine_tuned_model')
tokenizer.save_pretrained('./fine_tuned_model')







pip install rouge_score

# -*- coding: utf-8 -*-
"""Indigo Bert.ipynb

Automatically generated by Colab.
"""

# Install required libraries
!pip install datasets transformers nltk

# Import necessary libraries
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import pandas as pd
import re
from transformers import BertTokenizer, BertForQuestionAnswering, TrainingArguments, Trainer
from datasets import Dataset, load_metric
import numpy as np
import torch

# Download NLTK data
nltk.download('punkt')
nltk.download("stopwords")
nltk.download('wordnet')

# Load the dataset from Hugging Face
dataset = Dataset.from_pandas(pd.read_json("hf://datasets/toughdata/quora-question-answer-dataset/Quora-QuAD.jsonl", lines=True))

# Split the dataset into training and testing sets
split_dataset = dataset.train_test_split(test_size=0.2, seed=42)
train_dataset = split_dataset["train"]
test_dataset = split_dataset["test"]

# Print dataset sizes
print(f"Number of samples in training set: {len(train_dataset)}")
print(f"Number of samples in test set: {len(test_dataset)}")

# Define text cleaning function
def clean_text(text):
    text = text.lower()
    text = re.sub(r'\[.*?\]', '', text)
    text = re.sub(r'http\S+', '', text)
    text = re.sub(r'<.*?>+', '', text)
    text = re.sub(r'[^a-z\s]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

# Apply text cleaning
train_dataset = train_dataset.map(lambda x: {'question': clean_text(x['question']), 'answer': clean_text(x['answer'])})

# Tokenize text
train_dataset = train_dataset.map(lambda x: {
    'question_tokens': word_tokenize(x['question']),
    'answer_tokens': word_tokenize(x['answer'])
})

# Remove stopwords
stop_words = set(stopwords.words('english'))
train_dataset = train_dataset.map(lambda x: {
    'question_tokens': [word for word in x['question_tokens'] if word not in stop_words],
    'answer_tokens': [word for word in x['answer_tokens'] if word not in stop_words]
})

# Lemmatize tokens
lemmatizer = WordNetLemmatizer()
train_dataset = train_dataset.map(lambda x: {
    'question_tokens': [lemmatizer.lemmatize(token) for token in x['question_tokens']],
    'answer_tokens': [lemmatizer.lemmatize(token) for token in x['answer_tokens']]
})

# Convert token lists back to strings
train_dataset = train_dataset.map(lambda x: {
    'question_processed': ' '.join(x['question_tokens']),
    'answer_processed': ' '.join(x['answer_tokens'])
})

# Filter necessary columns
df2 = train_dataset.to_pandas()[['question_processed', 'answer_processed']]

# Load the tokenizer and model
model_name = 'bert-base-uncased'
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertForQuestionAnswering.from_pretrained(model_name)

# Preprocess function for tokenization
def preprocess_function(examples):
    inputs = tokenizer(
        examples['question_processed'],
        examples['answer_processed'],
        truncation='longest_first',
        padding='max_length',
        max_length=256,
        return_tensors='pt'
    )
    return inputs

# Convert the dataframe to a Dataset object and tokenize
dataset = Dataset.from_pandas(df2)
tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=['question_processed', 'answer_processed'])

# Load metrics
rouge = load_metric('rouge')
bleu = load_metric('bleu')

# F1 score computation
def compute_f1(preds, labels):
    preds = np.argmax(preds, axis=-1)
    true_positives = np.sum((preds == 1) & (labels == 1))
    false_positives = np.sum((preds == 1) & (labels == 0))
    false_negatives = np.sum((preds == 0) & (labels == 1))

    precision = true_positives / (true_positives + false_positives + 1e-10)
    recall = true_positives / (true_positives + false_negatives + 1e-10)
    f1 = 2 * (precision * recall) / (precision + recall + 1e-10)
    return f1

# Define the evaluation function
def compute_metrics(p):
    preds, labels = p
    preds = np.argmax(preds, axis=1)

    # Compute ROUGE
    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    rouge_result = rouge.compute(predictions=decoded_preds, references=decoded_labels)

    # Compute BLEU
    bleu_result = bleu.compute(predictions=[pred.split() for pred in decoded_preds], references=[[ref.split()] for ref in decoded_labels])

    # Compute F1-score
    f1_result = compute_f1(preds, labels)

    return {
        'rouge1': rouge_result['rouge1'].mid.fmeasure,
        'rouge2': rouge_result['rouge2'].mid.fmeasure,
        'rougeL': rouge_result['rougeL'].mid.fmeasure,
        'bleu': bleu_result['bleu'],
        'f1': f1_result
    }

# Fine-tune the model
training_args = TrainingArguments(
    output_dir='./results',
    evaluation_strategy="epoch",
    learning_rate=3e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=1,
    weight_decay=0.01,
    fp16=True,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    eval_dataset=tokenized_dataset,
    compute_metrics=compute_metrics,
)

trainer.train()

# Save the model
model.save_pretrained('./fine_tuned_model')
tokenizer.save_pretrained('./fine_tuned_model')

